{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/YogeshRathee512/Machine-learning-projects/blob/main/topsis%20implementation%20form%20scratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import Normalizer\n",
        "\n",
        "# Sample dataset loading (replace with your actual dataset path)\n",
        "data = pd.read_csv(\"/content/topsis_dataset.csv\")\n",
        "\n",
        "# Extract only the numeric columns for normalization\n",
        "numeric_columns = data.select_dtypes(include=[np.number]).columns\n",
        "\n",
        "# Create an instance of Normalizer\n",
        "scaler = Normalizer()\n",
        "\n",
        "def calculate_rank(distance_from_best,distance_from_worst):\n",
        "\n",
        "    ans = []\n",
        "    for elementOne,elementTwo in zip(distance_from_best,distance_from_worst):\n",
        "      ans.append(elementTwo/(elementOne+elementTwo))\n",
        "\n",
        "    return ans\n",
        "\n",
        "def Eucledian_distance(x, y):\n",
        "    return np.sqrt(np.sum((x - y)**2))\n",
        "\n",
        "def topsis(dataset, weights, impacts):\n",
        "    # Normalize only the numeric columns\n",
        "    normalized_data = scaler.fit_transform(dataset[numeric_columns])\n",
        "\n",
        "    # Replace the original numeric columns with the normalized values\n",
        "    dataset[numeric_columns] = normalized_data\n",
        "\n",
        "    # Display the dataset with normalized values\n",
        "    dataset = dataset.multiply(weights, axis=1)\n",
        "\n",
        "    print(\"Your dataset is\")\n",
        "    print(dataset)\n",
        "\n",
        "    count = 0\n",
        "    ideal_best = []\n",
        "    ideal_worst = []\n",
        "\n",
        "    for i in impacts:\n",
        "        if i == \"+\":\n",
        "            ideal_best.append(dataset.iloc[:, count].max())\n",
        "            ideal_worst.append(dataset.iloc[:, count].min())\n",
        "        else:\n",
        "            ideal_best.append(dataset.iloc[:, count].min())\n",
        "            ideal_worst.append(dataset.iloc[:, count].max())\n",
        "        count += 1  # Increment count for the next column\n",
        "\n",
        "    print(\"Ideal Best Values:\")\n",
        "    print(ideal_best)\n",
        "    print(\"Ideal Worst Values:\")\n",
        "    print(ideal_worst)\n",
        "\n",
        "    # Calculate Euclidean distance\n",
        "    no_of_rows = dataset.shape[0]\n",
        "    distance_from_best = []\n",
        "    distance_from_worst = []\n",
        "\n",
        "    for i in range(no_of_rows):\n",
        "        distance_from_best.append(Eucledian_distance(dataset.iloc[i, :], np.array(ideal_best)))\n",
        "        distance_from_worst.append(Eucledian_distance(dataset.iloc[i, :], np.array(ideal_worst)))\n",
        "\n",
        "    print(\"Distance from Best:\")\n",
        "    print(distance_from_best)\n",
        "    print(\"Distance from Worst:\")\n",
        "    print(distance_from_worst)\n",
        "\n",
        "    dataset[\"distance_from_best\"] = distance_from_best\n",
        "    dataset[\"distance_from_worst\"] = distance_from_worst\n",
        "\n",
        "    print(\"modified dataset is\")\n",
        "    print(dataset)\n",
        "\n",
        "    print(\"type of\")\n",
        "    print(type(distance_from_best))\n",
        "\n",
        "    arr = calculate_rank(distance_from_best,distance_from_worst)\n",
        "    print(\"performace score\")\n",
        "    print(arr)\n",
        "\n",
        "    dataset[\"performace_score\"] = arr\n",
        "\n",
        "\n",
        "    # Create a dictionary to store the ranks\n",
        "    rank_dict = {value: rank for rank, value in enumerate(sorted(arr, reverse=True), 1)}\n",
        "\n",
        "    # Use the dictionary to get ranks for each element in the original list\n",
        "    ranks = [rank_dict[value] for value in arr]\n",
        "\n",
        "    dataset[\"ranks\"] = ranks\n",
        "\n",
        "    print(ranks)\n",
        "\n",
        "    print(\"printing final output\")\n",
        "    print(dataset)\n",
        "\n",
        "\n",
        "# Define weights and impacts\n",
        "weights = np.ones(5) * 0.25\n",
        "impacts = [\"+\", \"-\", \"+\", \"-\", \"+\"]\n",
        "# Call the topsis function with your dataset\n",
        "topsis(data, weights, impacts)\n"
      ],
      "metadata": {
        "id": "z96GTS_HSiiq",
        "outputId": "999f0da9-71bd-4ef2-cf1d-d17cc4ed103c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Your dataset is\n",
            "         P1        P2        P3        P4        P5\n",
            "0  0.005043  0.003765  0.027566  0.238683  0.068781\n",
            "1  0.003274  0.002667  0.017379  0.240478  0.065960\n",
            "2  0.003771  0.003245  0.024116  0.239408  0.067657\n",
            "3  0.003503  0.002684  0.019109  0.240227  0.066381\n",
            "4  0.006911  0.006385  0.042817  0.235117  0.072789\n",
            "5  0.005095  0.004458  0.020844  0.239712  0.067513\n",
            "6  0.003318  0.002212  0.015082  0.240802  0.065353\n",
            "7  0.003220  0.002975  0.014700  0.240802  0.065415\n",
            "Ideal Best Values:\n",
            "[0.006910787520323515, 0.002211959571287102, 0.042816835723743514, 0.23511701020231088, 0.07278862073036398]\n",
            "Ideal Worst Values:\n",
            "[0.0032200239248666436, 0.006384966730733682, 0.014700109222217286, 0.24080196241966406, 0.0653533509698462]\n",
            "Distance from Best:\n",
            "[0.01634813119244091, 0.02712726093687753, 0.020134227514322138, 0.025319312029186393, 0.0041730071594465795, 0.02323939082959648, 0.029491642282348386, 0.0298568942880921]\n",
            "Distance from Worst:\n",
            "[0.013854996153342814, 0.00463379190189283, 0.010299346705944849, 0.005882112423947982, 0.029862584670527704, 0.007129840925006366, 0.004191547155175678, 0.0034105106876866076]\n",
            "modified dataset is\n",
            "         P1        P2        P3        P4        P5  distance_from_best  \\\n",
            "0  0.005043  0.003765  0.027566  0.238683  0.068781            0.016348   \n",
            "1  0.003274  0.002667  0.017379  0.240478  0.065960            0.027127   \n",
            "2  0.003771  0.003245  0.024116  0.239408  0.067657            0.020134   \n",
            "3  0.003503  0.002684  0.019109  0.240227  0.066381            0.025319   \n",
            "4  0.006911  0.006385  0.042817  0.235117  0.072789            0.004173   \n",
            "5  0.005095  0.004458  0.020844  0.239712  0.067513            0.023239   \n",
            "6  0.003318  0.002212  0.015082  0.240802  0.065353            0.029492   \n",
            "7  0.003220  0.002975  0.014700  0.240802  0.065415            0.029857   \n",
            "\n",
            "   distance_from_worst  \n",
            "0             0.013855  \n",
            "1             0.004634  \n",
            "2             0.010299  \n",
            "3             0.005882  \n",
            "4             0.029863  \n",
            "5             0.007130  \n",
            "6             0.004192  \n",
            "7             0.003411  \n",
            "type of\n",
            "<class 'list'>\n",
            "performace score\n",
            "[0.45872720379987186, 0.14589541239125461, 0.3384205427664189, 0.18852063734407765, 0.8773928427543453, 0.234771856681088, 0.12444032840031982, 0.10251808610168807]\n",
            "[2, 6, 3, 5, 1, 4, 7, 8]\n",
            "printing final output\n",
            "         P1        P2        P3        P4        P5  distance_from_best  \\\n",
            "0  0.005043  0.003765  0.027566  0.238683  0.068781            0.016348   \n",
            "1  0.003274  0.002667  0.017379  0.240478  0.065960            0.027127   \n",
            "2  0.003771  0.003245  0.024116  0.239408  0.067657            0.020134   \n",
            "3  0.003503  0.002684  0.019109  0.240227  0.066381            0.025319   \n",
            "4  0.006911  0.006385  0.042817  0.235117  0.072789            0.004173   \n",
            "5  0.005095  0.004458  0.020844  0.239712  0.067513            0.023239   \n",
            "6  0.003318  0.002212  0.015082  0.240802  0.065353            0.029492   \n",
            "7  0.003220  0.002975  0.014700  0.240802  0.065415            0.029857   \n",
            "\n",
            "   distance_from_worst  performace_score  ranks  \n",
            "0             0.013855          0.458727      2  \n",
            "1             0.004634          0.145895      6  \n",
            "2             0.010299          0.338421      3  \n",
            "3             0.005882          0.188521      5  \n",
            "4             0.029863          0.877393      1  \n",
            "5             0.007130          0.234772      4  \n",
            "6             0.004192          0.124440      7  \n",
            "7             0.003411          0.102518      8  \n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Welcome To Colaboratory",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}